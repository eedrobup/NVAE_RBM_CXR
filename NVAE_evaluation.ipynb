{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from rbm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from threelayerRBM import extract_features\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir, rbm_model, findings_list, hidden_features_dict,\n",
    "                 transform=None, vectorizer_v=None, hidden_vectorizers=None):\n",
    "        \"\"\"\n",
    "        csv_path: path to output.csv\n",
    "        root_dir: './files-1024'\n",
    "        rbm_model: an instance of ThreeLayerRBM already trained and loaded\n",
    "        findings_list, hidden_features_dict: from your threelayerRBM.py setup\n",
    "        transform: torchvision transforms for images\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.rbm = rbm_model\n",
    "        self.findings_list = findings_list\n",
    "        self.hidden_features_dict = hidden_features_dict\n",
    "\n",
    "        # Store vectorizers\n",
    "        self.vectorizer_v = vectorizer_v\n",
    "        self.hidden_vectorizers = hidden_vectorizers\n",
    "\n",
    "        # Pre-load vectorizers from your threelayerRBM code (assuming you saved them)\n",
    "        # If not saved, you must re-create them as done in extract_features()\n",
    "        # For simplicity, let's assume you have functions or preloaded vectorizers:\n",
    "        # self.vectorizer_v, self.hidden_vectorizers = load_vectorizers(...)\n",
    "\n",
    "        # To avoid repeated processing, we can store relevant info\n",
    "        # We'll store each row's path and text for lazy processing\n",
    "        self.samples = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            level1 = row['Level1']  # e.g. 'p10'\n",
    "            level2 = row['Level2']  # e.g. 'p10000032'\n",
    "            file_ = row['File']     # e.g. 's50414267'\n",
    "            # Construct the image directory\n",
    "            img_dir = os.path.join(self.root_dir, level1, level2, file_)\n",
    "\n",
    "            # Extract text\n",
    "            findings = str(row['FINDINGS']) if not pd.isnull(row['FINDINGS']) else ''\n",
    "            impression = str(row['IMPRESSION']) if not pd.isnull(row['IMPRESSION']) else ''\n",
    "            text = findings + ' ' + impression\n",
    "\n",
    "            self.samples.append((img_dir, text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # As in threelayerRBM.py\n",
    "        import re\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def get_features_from_text(self, clean_text, vectorizer_v, hidden_vectorizers):\n",
    "        # vectorizer_v is a CountVectorizer for findings\n",
    "        X_visible = vectorizer_v.transform([clean_text]).toarray()\n",
    "\n",
    "        hidden_features = []\n",
    "        for category, vec in hidden_vectorizers.items():\n",
    "            X_hidden_cat = vec.transform([clean_text]).toarray()\n",
    "            hidden_features.append(X_hidden_cat)\n",
    "        X_hidden = np.concatenate(hidden_features, axis=1) if len(hidden_features) > 0 else np.array([])\n",
    "\n",
    "        return X_visible, X_hidden\n",
    "    \n",
    "    def text_to_cond_z(self, text):\n",
    "        # Convert text to cond_z using RBM\n",
    "        # 1. Preprocess text\n",
    "        clean_text = self.preprocess_text(text)\n",
    "\n",
    "        # 2. Vectorize text to get X_visible and X_hidden (as in threelayerRBM.py extract_features step)\n",
    "        # Assuming you have vectorizers prepared:\n",
    "        # X_visible = self.vectorizer_v.transform([clean_text]).toarray()\n",
    "        # For hidden features:\n",
    "        # hidden_features = []\n",
    "        # for category, terms in self.hidden_features_dict.items():\n",
    "        #     X_hidden_cat = self.hidden_vectorizers[category].transform([clean_text]).toarray()\n",
    "        #     hidden_features.append(X_hidden_cat)\n",
    "        # X_hidden = np.concatenate(hidden_features, axis=1)\n",
    "\n",
    "        # If you do not have them preloaded, you must create vectorizers here or store them from training phase.\n",
    "\n",
    "        # For demonstration, assume you have a function get_features_from_text that returns X_visible and X_hidden:\n",
    "        X_visible, X_hidden = self.get_features_from_text(clean_text, self.vectorizer_v, self.hidden_vectorizers)\n",
    "        # 3. cond_z = rbm.transform(X_visible)\n",
    "        cond_z = torch.tensor(self.rbm.transform(X_visible), dtype=torch.float32)\n",
    "        \n",
    "        return cond_z[0]  # Since batch size = 1 here\n",
    "\n",
    "\n",
    "    def load_image(self, img_dir):\n",
    "        # Load one or multiple images from img_dir\n",
    "        # If there are multiple .jpg images, decide how to handle them\n",
    "        imgs = [f for f in os.listdir(img_dir) if f.lower().endswith('.jpg')]\n",
    "        if len(imgs) == 0:\n",
    "            raise FileNotFoundError(f\"No image found in {img_dir}\")\n",
    "\n",
    "        # For simplicity, load the first image\n",
    "        img_path = os.path.join(img_dir, imgs[0])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_dir, text = self.samples[idx]\n",
    "\n",
    "        img = self.load_image(img_dir)\n",
    "        cond_z = self.text_to_cond_z(text)\n",
    "        return img, cond_z\n",
    "\n",
    "from threelayerRBM import ThreeLayerRBM\n",
    "\n",
    "# Load your RBM model and vectorizers\n",
    "findings_list = [\n",
    "    \"left\", \"right\", \"atelectasis\", \"bronchiectasis\", \"bulla\", \"consolidation\", \"dextrocardia\", \"effusion\", \"emphysema\",\n",
    "    \"fracture clavicle\", \"fracture rib\", \"groundglass opacity\", \"interstitial opacification\",\n",
    "    \"mass paraspinal\", \"mass soft tissue\", \"nodule\", \"opacity\", \"pneumomediastinum\", \"pneumonia\",\n",
    "    \"pneumoperitoneum\", \"pneumothorax\", \"pleural effusion\", \"pulmonary edema\", \"scoliosis\",\n",
    "    \"tuberculosis\", \"volume loss\", \"rib\", \"mass\", \"infiltration\", \"other findings\"\n",
    "]\n",
    "\n",
    "hidden_features_dict = {\n",
    "    'location': [\n",
    "        \"left lung\", \"right lung\", \"upper lobe\", \"lower lobe\", \"cardiac region\",\n",
    "        \"pleural space\", \"diaphragm\", \"mediastinum\", \"thoracic spine\", \"abdominal region\"\n",
    "    ],\n",
    "    'organ_system': [\n",
    "        \"respiratory system\", \"cardiovascular system\", \"musculoskeletal system\", \"digestive system\"\n",
    "    ],\n",
    "    'mode_of_pathology': [\n",
    "        \"congenital\", \"acquired\", \"infection\", \"inflammation\", \"tumor\", \"degenerative\", \"vascular\"\n",
    "    ],\n",
    "    'severity': [\n",
    "        \"mild\", \"moderate\", \"severe\",\n",
    "    ],\n",
    "}\n",
    "df = pd.read_csv('output.csv')\n",
    "X_visible, X_hidden, vectorizer_v = extract_features(df, findings_list, hidden_features_dict)\n",
    "\n",
    "n_visible = X_visible.shape[1]\n",
    "n_hidden_middle = 30\n",
    "n_hidden_top = X_hidden.shape[1]\n",
    "rbm = ThreeLayerRBM(n_visible, n_hidden_middle, n_hidden_top)\n",
    "rbm.load_model('rbm_model.pkl')\n",
    "with open('vectorizer_v.pkl', 'rb') as f:\n",
    "    vectorizer_v = pickle.load(f)\n",
    "with open('hidden_vectorizers.pkl', 'rb') as f:\n",
    "    hidden_vectorizers = pickle.load(f)\n",
    "\n",
    "# transforms for images\n",
    "transform = T.Compose([\n",
    "    T.Resize((256,256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ChestXRayDataset(\n",
    "    csv_path='output.csv',\n",
    "    root_dir='./files-1024',\n",
    "    rbm_model=rbm,\n",
    "    findings_list=findings_list,\n",
    "    hidden_features_dict=hidden_features_dict,\n",
    "    transform=transform,\n",
    "    vectorizer_v=vectorizer_v,\n",
    "    hidden_vectorizers=hidden_vectorizers\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from rbm_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/kf17dgqj3hz1bn64hxwjzxv80000gn/T/ipykernel_27180/3837004821.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len log norm: 128\n",
      "len bn: 92\n",
      "Average KL divergence over first 500 samples: 0.33895698574185373\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from model import AutoEncoder\n",
    "import re\n",
    "from distributions import Normal, DiscMixLogistic, NormalDecoder\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_features_from_text(clean_text, vectorizer_v, hidden_vectorizers):\n",
    "    # vectorizer_v is a CountVectorizer for findings\n",
    "    X_visible = vectorizer_v.transform([clean_text]).toarray()\n",
    "\n",
    "    hidden_features = []\n",
    "    for category, vec in hidden_vectorizers.items():\n",
    "        X_hidden_cat = vec.transform([clean_text]).toarray()\n",
    "        hidden_features.append(X_hidden_cat)\n",
    "    X_hidden = np.concatenate(hidden_features, axis=1) if len(hidden_features) > 0 else np.array([])\n",
    "\n",
    "    return X_visible, X_hidden\n",
    "\n",
    "def sample(model, num_samples, t, cond_z=None):\n",
    "    scale_ind = 0\n",
    "    z0_size = [num_samples] + model.z0_size\n",
    "    dist = Normal(mu=torch.zeros(z0_size), log_sigma=torch.zeros(z0_size), temp=t)\n",
    "    z, _ = dist.sample()\n",
    "\n",
    "    idx_dec = 0\n",
    "    s = model.prior_ftr0.unsqueeze(0)\n",
    "    batch_size = z.size(0)\n",
    "    s = s.expand(batch_size, -1, -1, -1)\n",
    "\n",
    "    # If conditioning is provided\n",
    "    if cond_z is not None and model.cond_z_dim > 0:\n",
    "        # cond_z shape: [num_samples, cond_z_dim]\n",
    "        # We'll apply this shift whenever we form mu, log_sigma from dec_sampler\n",
    "        shift_all = model.cond_mapper(cond_z)  # [B, 2*latent_per_group]\n",
    "        shift_all = shift_all.unsqueeze(-1).unsqueeze(-1)  # [B, 2*latent_per_group, 1, 1]\n",
    "\n",
    "    for cell in model.dec_tower:\n",
    "        if cell.cell_type == 'combiner_dec':\n",
    "            if idx_dec > 0:\n",
    "                # form prior\n",
    "                param = model.dec_sampler[idx_dec - 1](s)\n",
    "                mu, log_sigma = torch.chunk(param, 2, dim=1)\n",
    "                dist = Normal(mu, log_sigma, t)\n",
    "                z, _ = dist.sample()\n",
    "\n",
    "            # 'combiner_dec'\n",
    "            s = cell(s, z)\n",
    "            idx_dec += 1\n",
    "        else:\n",
    "            s = cell(s)\n",
    "            if cell.cell_type == 'up_dec':\n",
    "                scale_ind += 1\n",
    "\n",
    "    if model.vanilla_vae:\n",
    "        s = model.stem_decoder(z)\n",
    "\n",
    "    for cell in model.post_process:\n",
    "        s = cell(s)\n",
    "\n",
    "    logits = model.image_conditional(s)\n",
    "    return logits\n",
    "\n",
    "# Load model, rbm, vectorizers\n",
    "# Assume you have a function load_model and it returns a model that can do model.sample(num_samples, cond_z=...)\n",
    "def load_model(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    args = checkpoint['args']\n",
    "    arch_instance = checkpoint['arch_instance']\n",
    "    model = AutoEncoder(args, None, arch_instance, cond_z_dim=24)  # fill in cond_z_dim\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    model.eval()\n",
    "    return model, args\n",
    "\n",
    "# Assume you have vectorizer_v, hidden_vectorizers, rbm loaded\n",
    "with open('vectorizer_v.pkl', 'rb') as f:\n",
    "    vectorizer_v = pickle.load(f)\n",
    "with open('hidden_vectorizers.pkl', 'rb') as f:\n",
    "    hidden_vectorizers = pickle.load(f)\n",
    "rbm = ThreeLayerRBM(n_visible, n_hidden_middle, n_hidden_top)\n",
    "rbm.load_model('rbm_model.pkl')\n",
    "\n",
    "model, args = load_model('./eval-exp64-12000-7epoch/checkpoint.pt')\n",
    "\n",
    "# Read cond_z from output.csv\n",
    "df = pd.read_csv('output.csv')\n",
    "# We'll assume df has Level1, Level2, File, FINDINGS, IMPRESSION columns\n",
    "# We'll generate cond_z for first 5 samples\n",
    "cond_zs = []\n",
    "for i in range(25):\n",
    "    row = df.iloc[i]\n",
    "    findings = str(row['FINDINGS']) if pd.notnull(row['FINDINGS']) else ''\n",
    "    impression = str(row['IMPRESSION']) if pd.notnull(row['IMPRESSION']) else ''\n",
    "    text = findings + ' ' + impression\n",
    "    clean_text = preprocess_text(text)\n",
    "    X_visible, X_hidden = get_features_from_text(clean_text, vectorizer_v, hidden_vectorizers)\n",
    "    cond_z = torch.tensor(rbm.transform(X_visible), dtype=torch.float32).unsqueeze(0)  # [1, cond_z_dim]\n",
    "    cond_zs.append(cond_z)\n",
    "\n",
    "cond_zs = torch.cat(cond_zs, dim=0)  # shape [25, cond_z_dim]\n",
    "\n",
    "# Generate first 5 images from cond_z\n",
    "# Assume model.sample(num_samples, t=1.0, cond_z=...) can handle condition\n",
    "# If model needs cond_z per image, loop or modify model to accept a batch of cond_z\n",
    "with torch.no_grad():\n",
    "    # For demonstration, let's say model.sample(num_samples=5, cond_z=cond_zs) works\n",
    "    logits = sample(model,num_samples=25, t=1.0, cond_z=cond_zs)  \n",
    "    # Decode logits\n",
    "    output = model.decoder_output(logits)\n",
    "    # output.sample() to get images\n",
    "    gen_images = output.sample().cpu()  # [25, C, H, W]\n",
    "\n",
    "# Assume original images are from the dataset. Load first 5 corresponding originals\n",
    "# You must know how to map them. We assume df also has paths or we replicate the logic from dataset.\n",
    "def load_original_image(row):\n",
    "    level1 = row['Level1']\n",
    "    level2 = row['Level2']\n",
    "    file_ = row['File']\n",
    "    img_dir = os.path.join('./files-1024', str(level1), str(level2), str(file_))\n",
    "    imgs = [f for f in os.listdir(img_dir) if f.lower().endswith('.jpg')]\n",
    "    img_path = os.path.join(img_dir, imgs[0])\n",
    "    img = Image.open(img_path).convert('L') # assume grayscale\n",
    "    transform = T.Compose([\n",
    "        T.Resize((64,64)),  # match training resolution\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "#originals = []\n",
    "#for i in range(25):\n",
    "#    row = df.iloc[i]\n",
    "#    orig_img = load_original_image(row)\n",
    "#    originals.append(orig_img)\n",
    "#originals = torch.stack(originals, dim=0)  # [5, 1, 32,32]\n",
    "\n",
    "# 4) Create a 5x5 montage for original images\n",
    "#original_montage = vutils.make_grid(originals[:25], nrow=5, padding=2, normalize=True)\n",
    "#vutils.save_image(original_montage, 'original_montage.png')\n",
    "\n",
    "# 5) Create a 5x5 montage for generated images\n",
    "generated_montage = vutils.make_grid(gen_images[:25], nrow=5, padding=2, normalize=True)\n",
    "vutils.save_image(generated_montage, 'generated_montage.png')\n",
    "\n",
    "# 6) calculate KL divergence of the first 500 original and associated generated images\n",
    "# We must define a KL divergence measure. We'll assume both original and generated are normalized\n",
    "# distributions over pixels. This is simplistic and not necessarily meaningful, but as an example:\n",
    "\n",
    "def kl_divergence(p, q, eps=1e-8):\n",
    "    # p, q are [C,H,W]. Ensure sum to 1 if treating as distributions\n",
    "    # We'll sum over all pixels. Let's flatten them.\n",
    "    # This is not a standard approach for images, but just a demonstration.\n",
    "    p = p.flatten()\n",
    "    q = q.flatten()\n",
    "    # Normalize to sum=1\n",
    "    p = p / (p.sum() + eps)\n",
    "    q = q / (q.sum() + eps)\n",
    "    kl = (p * (torch.log(p+eps) - torch.log(q+eps))).sum()\n",
    "    return kl.item()\n",
    "\n",
    "# We'll load first 500 samples (or as many as we have)\n",
    "num_samples_kl = min(len(df), 500)\n",
    "kls = []\n",
    "orig_list = []\n",
    "gen_list = []\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples_kl):\n",
    "        row = df.iloc[i]\n",
    "        # get cond_z\n",
    "        findings = str(row['FINDINGS']) if pd.notnull(row['FINDINGS']) else ''\n",
    "        impression = str(row['IMPRESSION']) if pd.notnull(row['IMPRESSION']) else ''\n",
    "        text = findings + ' ' + impression\n",
    "        clean_text = preprocess_text(text)\n",
    "        X_visible, X_hidden = get_features_from_text(clean_text, vectorizer_v, hidden_vectorizers)\n",
    "        cond_z = torch.tensor(rbm.transform(X_visible), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Generate image for this cond_z\n",
    "        logits = sample(model,num_samples=1, t=1.0, cond_z=cond_z)\n",
    "        out = model.decoder_output(logits)\n",
    "        gen_img = out.sample()[0]  # [C,H,W]\n",
    "\n",
    "        orig_img = load_original_image(row) # [1,H,W]\n",
    "        # Convert both to CPU float\n",
    "        orig_img = orig_img.cpu().float()\n",
    "        gen_img = gen_img.cpu().float()\n",
    "        orig_list.append(orig_img)\n",
    "        gen_list.append(torch.mean(gen_img,dim=0))\n",
    "\n",
    "        kl_value = kl_divergence(orig_img, torch.mean(gen_img,dim=0))\n",
    "        kls.append(kl_value)\n",
    "\n",
    "# 7) report an average of calculated KL divergence\n",
    "avg_kl = sum(kls) / len(kls)\n",
    "print(\"Average KL divergence over first 500 samples:\", avg_kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paussava/miniconda3/envs/rama/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/paussava/miniconda3/envs/rama/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Score: tensor(1.5833)\n",
      "FID: 328.8058996039571\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# --------------- Calculate Inception Score for generated images ----------------\n",
    "# Inception Score only on gen_batch\n",
    "\n",
    "def inception_score(imgs, splits=10):\n",
    "    # imgs: [N,3,299,299]\n",
    "    N = imgs.size(0)\n",
    "    batch_size = 50\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, N, batch_size):\n",
    "            batch = imgs[i:i+batch_size]\n",
    "            out = inception(batch)\n",
    "            # out: logits before softmax. Get probabilities:\n",
    "            p_yx = F.softmax(out, dim=1)\n",
    "            preds.append(p_yx)\n",
    "    preds = torch.cat(preds, dim=0) # [N,1000]\n",
    "    # Compute IS\n",
    "    # split into 10 groups\n",
    "    split_size = N // splits\n",
    "    is_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k*split_size:(k+1)*split_size, :]\n",
    "        py = part.mean(dim=0)\n",
    "        # KL divergence: mean over part of sum p(y|x)*log(p(y|x)/p(y))\n",
    "        scores = (part * (torch.log(part+1e-8) - torch.log(py+1e-8))).sum(dim=1).mean()\n",
    "        is_scores.append(torch.exp(scores))\n",
    "    return sum(is_scores)/len(is_scores)\n",
    "\n",
    "# --------------- Calculate FID ----------------\n",
    "# For FID, we need activations from a layer of inception (often pool3 features)\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "def get_activations(imgs, model, batch_size=50):\n",
    "    # Extract pool3 features\n",
    "    # Modify inception to output features before fc:\n",
    "    # For simplicity, use model until pool3:\n",
    "    model.Mixed_7c.register_forward_hook(lambda m,i,o: setattr(model,'_hidden',o))\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, imgs.size(0), batch_size):\n",
    "            batch = imgs[i:i+batch_size]\n",
    "            _ = model(batch) \n",
    "            # model._hidden now has features [B,2048,H',W']\n",
    "            # pool to 1x1\n",
    "            feat = adaptive_avg_pool2d(model._hidden, (1,1)).squeeze(-1).squeeze(-1)\n",
    "            activations.append(feat)\n",
    "    activations = torch.cat(activations, dim=0)\n",
    "    return activations.cpu().numpy()\n",
    "\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    # from official FID formula\n",
    "    diff = mu1 - mu2\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if not np.isfinite(covmean).all():\n",
    "        covmean = np.nan_to_num(covmean)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "\n",
    "\n",
    "def compute_statistics(acts):\n",
    "    mu = np.mean(acts, axis=0)\n",
    "    sigma = np.cov(acts, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def inception_preprocess(img):\n",
    "    # img is a torch.Tensor with shape either:\n",
    "    # - [H,W]\n",
    "    # - [1,H,W]\n",
    "    # We want to end up with [3,299,299]\n",
    "\n",
    "    # If img is [H,W], add a channel dimension\n",
    "    if img.dim() == 2:\n",
    "        # shape: [H,W] -> [1,H,W]\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # If img has only one channel, replicate it to get 3 channels\n",
    "    if img.size(0) == 1:\n",
    "        img = img.repeat(3,1,1)  # [1,H,W] -> [3,H,W]\n",
    "\n",
    "    # Now we have [3,H,W]. Convert to PIL, then apply resizing & normalization\n",
    "    pil = T.ToPILImage()(img)  # Convert tensor to PIL\n",
    "    transform = T.Compose([\n",
    "        T.Resize((299,299)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    ])\n",
    "    img = transform(pil)\n",
    "    return img\n",
    "\n",
    "orig_batch = torch.stack([inception_preprocess(im) for im in orig_list], dim=0)\n",
    "gen_batch = torch.stack([inception_preprocess(im) for im in gen_list], dim=0)\n",
    "\n",
    "inception = inception_v3(pretrained=True, transform_input=False).eval()\n",
    "\n",
    "IS = inception_score(gen_batch, splits=10)\n",
    "print(\"Inception Score:\", IS)\n",
    "\n",
    "activations_real = get_activations(orig_batch, inception)\n",
    "activations_fake = get_activations(gen_batch, inception)\n",
    "\n",
    "mu_r, sigma_r = compute_statistics(activations_real)\n",
    "mu_g, sigma_g = compute_statistics(activations_fake)\n",
    "\n",
    "FID = calculate_fid(mu_r, sigma_r, mu_g, sigma_g)\n",
    "print(\"FID:\", FID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
